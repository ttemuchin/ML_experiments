{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0:\n",
      "Yi: [0.0, 0.0]\n",
      "X[0]: first 5 values - [0.99637568, 1.023678303, 0.9975966215, 0.9909123182, 1.023943067]\n",
      "X[1]: first 5 values - [-0.009428942576, 0.2132565677, 0.3970399499, 0.5280015469, 0.631996572]\n",
      "\n",
      "Sample 1:\n",
      "Yi: [1.0, 1.0]\n",
      "X[0]: first 5 values - [1.000395894, 0.9988130927, 0.9877015352, 0.9881074429, 0.9872054458]\n",
      "X[1]: first 5 values - [0.0006334115169, 0.05415157974, 0.09516408294, 0.1379769295, 0.1814425886]\n",
      "\n",
      "[0.99637568, 1.023678303, 0.9975966215, 0.9909123182, 1.023943067, 1.000510573, 0.9967061281, 0.9915313125, 0.9672198892, 1.018810511, 0.9488478899, 0.9793912172, 0.9582349658, 0.9980856776, 0.9709777832, 0.9585749507, 0.9518962502, 0.9515383244, 0.9456630945, 0.9394155145, 0.9190992713, 0.9366926551, 0.9262087345, 0.911018908, 0.9323714972, 0.8748521209, 0.8935570717, 0.8939537406, 0.8839122057, 0.8600383997, 0.8556703925, 0.8464894295, 0.885545671, 0.8065662384, 0.8185435534, 0.7923355103, 0.7976151109, 0.7660022378, 0.8178522587, 0.7531133294, 0.7159899473, 0.7413577437, 0.7132852077, 0.7521454096, 0.7339141965, 0.6819659472, 0.7179055214, 0.6554280519, 0.6437063217, 0.6604865193, 0.6487700939, 0.61067307, 0.6118933558, 0.615121901, 0.6035858989, 0.5954194069, 0.5852798223, 0.5699025989, 0.5279788971, 0.5375980139, 0.5015287995, 0.555627048, 0.5116599798, 0.4853783846, 0.4892786741, 0.5244634151, 0.5041868091, 0.4676781595, 0.4503016472, 0.4420560002, 0.434785217, 0.4173340499, 0.3919159472, 0.4114314914, 0.3586443365, 0.3478958607, 0.3689263165, 0.374951154, 0.356969595, 0.3791398108, 0.3375326097, 0.350089848, 0.3436894417, 0.3015006781, 0.2938825488, 0.2554580271, 0.2839168906, 0.2212784588, 0.2787973285, 0.2118753493, 0.2457542121, 0.2127660066, 0.2467051744, 0.1985830665, 0.210063532, 0.2096465379, 0.1774920076, 0.1964276731, 0.1875909269, 0.1357237399, 0.1967666894, 0.2166726589, 0.2018239647, 0.1319943964, 0.1156385094, 0.1054750979, 0.1420273185, 0.137786299, 0.1352219284, 0.08738010377, 0.09673108906, 0.1259707659, 0.1149745211, 0.1377625465, 0.1027042717, 0.1080370545, 0.0972661972, 0.140593037, 0.06195988506, 0.06585200876, 0.1001049355, 0.1009543911, 0.08198480308, 0.03964788467, 0.06646954268, 0.0667899847, 0.05550024286, 0.1082671285, 0.07088283449, 0.06291034073, 0.05526920408, 0.03680086881, 0.02970843576, 0.04677014425, 0.04430984706, 0.04361073673, 0.05417427421, 0.08859604597, 0.01284494624, 0.04106145352, 0.02312411927, 0.02819075435, 0.05791050568, 0.02048086375, 0.01692236401, 0.03146500513, 0.03645295277, -0.0006434558309, -0.01584759168, 0.06164532155, 0.02425125428, 0.02153674327, -0.009947031736, -0.02175589837, 0.04084989801, 0.05783997104, 0.00652284408, 0.02113634534, -0.02753864601, 0.01215830352, -0.02030174434, 0.01615022495, 0.01523423567, 0.01122637279, -0.0222402215, 0.01692759432, 0.0040263501, 0.006634061225, -0.002440216485, 0.008712301962, -0.03433246166, 0.00506220758, -0.00300550065, 0.008269354701, 0.02527967282, 0.009243267588, -0.04287172481, 0.01096299384, 0.005587751977, 0.003909708932, -0.01955743879, 0.003594011534, -0.03265086934, -0.0411038436, 0.01873412728, 0.038634222, -0.02750635892, 0.0001852104033, -0.009169055149, -0.02009315602, -0.0002762108634, 0.0307511948, 0.002855355851, 0.002498888411, -0.01011294965, 0.008917249739, 0.02701687627, 0.03050786071, 0.003765940666, 0.000879639003, 0.02912584879, 0.0209958218, 0.01395784412, 0.03604404256, -0.01539879385, 0.04079780355, 0.01471648552, -0.02361369133, 0.0189999789, 0.008651637472, -0.01857410371, -0.03679910302, -0.003912726417, 0.04586867616, 0.02124665119, 0.0001411007397, -0.04339133948, -0.01868696697, -0.000717399409, 0.004380413797, 0.002811760642, -0.04340255633, -0.03350599483, 0.02668745629, -0.00988716539, -0.01885223761, -0.005351867992, -0.04269982502, 0.004567895085, -0.03897174448, -0.01345380209, 0.01009996515, -0.003770667594, -0.01925778016, 0.001341745607, -0.04629378021, 0.02705322951, 0.04341477901, -0.007552953437, 0.01551930234, 0.01718579419, 0.002232665429, -0.03868826851, 0.03663500026, 0.0004733695532, 0.02997270226, 0.01256243885, 0.0002946720051, 7.969145372e-05, 0.04819980636, -0.01410820521, -0.02084026299, -0.0001028792904, -0.008021334186, 0.01065089647, 0.03014412522, -0.01890830509, -0.001097559812, -0.01684481651, -0.008900917135, -0.0009560424369, 0.03368093446, 0.003471484873, 0.001294217887, -0.02121883444, 0.004934405442, -0.01504875813, 0.0022126392, -0.009966841899, 0.0004421708873, 0.02624181658, 0.03340582922, -0.00691109756, -0.00236844644, -0.04526340589, -0.0296126809, 0.03291304037, -0.0003073710832, 0.002687671687, -0.001660872251, -0.02388489619, -0.0002813808096, -0.001169855124, -0.02213534899, -0.02037986368, 0.007426151074, 0.01883712038, -0.0002290859411, -0.01369000971, -0.0008326016832, 2.023817797e-05, 0.007186629344, 0.03880691528, -0.003289139597, -0.003430244513, -0.04188303649, -0.00016314807, -0.007202558685, 0.006730271969, 0.0229289867, -0.004696461838, 0.0313244313, 0.04905537888, -0.008811136708, 0.0004068161652, -0.005514343735, 0.001337477355, 0.002448196989, 0.002124452498, 0.03684550524, 0.02438998967, -0.04633777961, -0.002103322651, -0.006253545638, -0.02278402261, -0.002611305099, -0.01020810567, -0.001043409109, 0.0007941196673, 0.02857465297, 0.01950437762, -0.03421029821, 0.02732067183, 0.000679068733, -0.007099221926, -0.001960560214, -0.008680074476, 2.256437074e-05, -0.0002149719658, 0.01957873814, -0.04636600614, -0.02985845879, 0.0212109331, -0.0002385871921, 0.003214822616, -0.04787631705, -0.008909889497, 0.007988497615, -0.02263141237, 0.02123289555, 0.04066643491, 0.00443649618, -0.01139433589, 0.0001330219675, -0.002948991721, -0.0001221709535, -0.0006089729141, -0.001732381177, 0.00189932168, -0.00035337999, 0.03169045597, 0.03391105682, -0.004538842477, -0.005887691863, 0.03323677927, 0.005291105248, 0.01905407012, -0.00525320787, 0.04498937726, 0.02384292893, 0.02043731883, 0.0002588024072, 0.003233578056, -0.002305907663, 0.02616780065, -0.008658071049, -0.002225236036, 0.01358198375, -0.04811447114, -0.007056053728, -0.03007779084, 0.0005281979684, 0.01650466584, -0.02209939994, -0.04733170941, -0.02623875067, -0.03550577536, 0.0009030374931, -0.007316195872, 0.03888563439, -0.0006470907247, 0.04421058297, -0.003881507087, 0.002902941545, 0.01352071669, 0.006734614726, -0.03681021184, 0.005884671584, -0.0003835257667, -0.003982759081, -0.01214660611, -0.02286762372, -1.535229603e-05, 4.641722626e-06, 0.004686777946, 0.01359061431, -0.01318768878, -0.0001459767664, -0.02880212106, 0.0005620708107]\n",
      "1000\n",
      "2\n",
      "400\n",
      "60\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# nY= 2\n",
    "# nX= 2\n",
    "# Y[0]= 1\n",
    "# Y[1]= 0.25\n",
    "# array of X[0] with 400 elements\n",
    "#     8.193433843E-8\n",
    "# array of X[1] with 60 elements\n",
    "# 3.685414961E-43\n",
    "# 0.07688365132\n",
    "import os\n",
    "\n",
    "def parse_directory(directory_path):\n",
    "    all_samples = []\n",
    "    \n",
    "    # Проверяем, что директория существует\n",
    "    if not os.path.isdir(directory_path):\n",
    "        raise ValueError(f\"Directory {directory_path} does not exist\")\n",
    "    \n",
    "    # Проходим по всем файлам в директории\n",
    "    for filename in os.listdir(directory_path):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        \n",
    "        # Пропускаем поддиректории и не-файлы\n",
    "        if not os.path.isfile(file_path):\n",
    "            continue\n",
    "            \n",
    "        # Парсим каждый файл и добавляем образцы в общий массив\n",
    "        try:\n",
    "            file_samples = parse_data_file(file_path)\n",
    "            all_samples.extend(file_samples)\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing file {filename}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return all_samples\n",
    "\n",
    "def parse_data_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Извлекаем количество образцов A из первой строки\n",
    "    try:\n",
    "        first_line = lines[1].strip()\n",
    "        A = int(first_line.split('with ')[1].split(' records')[0])\n",
    "    except:\n",
    "        print(\"test data\")\n",
    "    samples = []\n",
    "    current_sample = None\n",
    "    current_x_key = None\n",
    "    collecting_x_data = False\n",
    "    x_data_buffer = []\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        \n",
    "        # Начало нового образца\n",
    "        if \"*******************new*record*******************\" in line:\n",
    "            if current_sample is not None:\n",
    "                # Завершаем предыдущий образец, если есть\n",
    "                if current_x_key is not None and x_data_buffer:\n",
    "                    current_sample[current_x_key] = x_data_buffer\n",
    "                samples.append(current_sample)\n",
    "            \n",
    "            current_sample = {\"Yi\": []}\n",
    "            current_x_key = None\n",
    "            collecting_x_data = False\n",
    "            x_data_buffer = []\n",
    "            continue\n",
    "        \n",
    "        # Обработка nY (количество целевых переменных)\n",
    "        if line.startswith(\"nY=\"):\n",
    "            nY = int(line.split('=')[1].strip())\n",
    "            # Это значение уже должно соответствовать количеству Y-переменных\n",
    "            continue\n",
    "        \n",
    "        # Обработка строк с Y-переменными\n",
    "        if line.startswith(\"Y\"):\n",
    "            parts = line.split('=')\n",
    "            if len(parts) == 2:\n",
    "                try:\n",
    "                    y_value = float(parts[1].strip())\n",
    "                    current_sample[\"Yi\"].append(y_value)\n",
    "                except ValueError:\n",
    "                    pass\n",
    "            continue\n",
    "        \n",
    "        # Обработка nX (количество экспериментов)\n",
    "        if line.startswith(\"nX=\"):\n",
    "            nX = int(line.split('=')[1].strip())\n",
    "            # Инициализируем ключи для экспериментов\n",
    "            for i in range(nX):\n",
    "                current_sample[f\"X[{i}]\"] = []\n",
    "            continue\n",
    "        \n",
    "        # Обработка начала массива эксперимента\n",
    "        if \"array of X[\" in line and \"with\" in line:\n",
    "            # Завершаем сбор предыдущих данных, если они есть\n",
    "            if current_x_key is not None and x_data_buffer:\n",
    "                current_sample[current_x_key] = x_data_buffer\n",
    "                x_data_buffer = []\n",
    "            \n",
    "            # Извлекаем номер эксперимента\n",
    "            x_index = line.split('array of X[')[1].split(']')[0]\n",
    "            current_x_key = f\"X[{x_index}]\"\n",
    "            collecting_x_data = True\n",
    "            continue\n",
    "        \n",
    "        # Сбор данных эксперимента\n",
    "        if collecting_x_data:\n",
    "            # Пытаемся извлечь числа из строки\n",
    "            numbers = []\n",
    "            for part in line.split():\n",
    "                try:\n",
    "                    num = float(part)\n",
    "                    numbers.append(num)\n",
    "                except ValueError:\n",
    "                    pass\n",
    "            x_data_buffer.extend(numbers)\n",
    "    \n",
    "    # Добавляем последний образец, если он есть\n",
    "    if current_sample is not None:\n",
    "        if current_x_key is not None and x_data_buffer:\n",
    "            current_sample[current_x_key] = x_data_buffer\n",
    "        samples.append(current_sample)\n",
    "    \n",
    "    # Проверяем, что количество образцов соответствует A\n",
    "    try:\n",
    "        if len(samples) != A:\n",
    "            print(f\"Warning: Expected {A} samples, but found {len(samples)}\")\n",
    "    finally:\n",
    "        return samples\n",
    "\n",
    "# Пример использования\n",
    "file_path = \"../kozlopan_ml1000noise.txt\"\n",
    "parsed_data = parse_data_file(file_path)\n",
    "\n",
    "# Вывод первых двух образцов для проверки\n",
    "for i, sample in enumerate(parsed_data[:2]):\n",
    "    print(f\"Sample {i}:\")\n",
    "    print(f\"Yi: {sample.get('Yi', [])}\")\n",
    "    for key in sorted(sample.keys()):\n",
    "        if key.startswith(\"X[\"):\n",
    "            print(f\"{key}: first 5 values - {sample[key][:5]}\")\n",
    "    print()\n",
    "\n",
    "print(parsed_data[0][\"X[0]\"])\n",
    "print(len(parsed_data))\n",
    "print(len(parsed_data[0][\"Yi\"]))\n",
    "print(len(parsed_data[0][\"X[0]\"]))\n",
    "print(len(parsed_data[0][\"X[1]\"]))\n",
    "print(len(parsed_data[0])-1)\n",
    "# print(parsed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../kozlopan_signal.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../kozlopan_signal.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m test_data \u001b[38;5;241m=\u001b[39m \u001b[43mparse_data_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(test_data)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(test_data[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX[0]\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[1;32mIn[3], line 38\u001b[0m, in \u001b[0;36mparse_data_file\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_data_file\u001b[39m(file_path):\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m     39\u001b[0m         lines \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mreadlines()\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# Извлекаем количество образцов A из первой строки\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../kozlopan_signal.txt'"
     ]
    }
   ],
   "source": [
    "file_path = \"../kozlopan_signal.txt\"\n",
    "test_data = parse_data_file(file_path)\n",
    "print(test_data)\n",
    "print(test_data[0][\"X[0]\"])\n",
    "print(len(test_data))\n",
    "print(len(test_data[0][\"Yi\"]))\n",
    "print(len(test_data[0][\"X[0]\"]))\n",
    "print(len(test_data[0][\"X[1]\"]))\n",
    "print(len(test_data[0])-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data\n",
      "test data\n",
      "test data\n",
      "test data\n",
      "test data\n",
      "test data\n",
      "test data\n",
      "test data\n",
      "test data\n",
      "test data\n",
      "test data\n",
      "test data\n",
      "test data\n",
      "test data\n",
      "test data\n",
      "test data\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "parsed_data_for_test = parse_directory(directory_path=\"../SygnalsWithoutNoise\")\n",
    "print(len(parsed_data_for_test))\n",
    "# print(parsed_data_for_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего образцов: 1000\n",
      "Обучающая выборка: 800 образцов\n",
      "Тестовая выборка: 200 образцов\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "def split_data(parsed_data, train_ratio=0.8, shuffle=True, random_seed=None):\n",
    "    \"\"\"\n",
    "    Разделяет данные на обучающую и тестовую выборки\n",
    "    \n",
    "    :param parsed_data: Исходные данные (массив словарей)\n",
    "    :param train_ratio: Доля обучающих данных (по умолчанию 0.8)\n",
    "    :param shuffle: Флаг перемешивания данных перед разделением\n",
    "    :param random_seed: Фиксирует случайность для воспроизводимости\n",
    "    :return: train_data, test_data (оба в том же формате, что и parsed_data)\n",
    "    \"\"\"\n",
    "    # Создаем копию данных, чтобы не изменять оригинал\n",
    "    data_copy = deepcopy(parsed_data)\n",
    "    \n",
    "    # Фиксируем случайность при необходимости\n",
    "    if random_seed is not None:\n",
    "        random.seed(random_seed)\n",
    "    \n",
    "    # Перемешиваем данные, если требуется\n",
    "    if shuffle:\n",
    "        random.shuffle(data_copy)\n",
    "    \n",
    "    # Определяем индекс разделения\n",
    "    split_idx = int(len(data_copy) * train_ratio)\n",
    "    \n",
    "    # Разделяем данные\n",
    "    train_data = data_copy[:split_idx]\n",
    "    test_data = data_copy[split_idx:]\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "# Пример использования:\n",
    "train_data, test_data = split_data(parsed_data, train_ratio=0.8, shuffle=True, random_seed=42)\n",
    "\n",
    "print(f\"Всего образцов: {len(parsed_data)}\")\n",
    "print(f\"Обучающая выборка: {len(train_data)} образцов\")\n",
    "print(f\"Тестовая выборка: {len(test_data)} образцов\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitSamples(parsed_data_list):\n",
    "    # Извлечение данных из parsed_data\n",
    "    y_data = [sample[\"Yi\"] for sample in parsed_data_list]\n",
    "    x_data = []\n",
    "\n",
    "    # Собираем данные по каждому эксперименту\n",
    "    for i in range(len(parsed_data_list[0])-1):\n",
    "        x_key = f\"X[{i}]\"\n",
    "        x_data_i = [sample.get(x_key, []) for sample in parsed_data_list]\n",
    "        x_data.append(x_data_i)\n",
    "\n",
    "    # Проверяем, что все массивы имеют одинаковую длину\n",
    "    assert all(len(x) == len(y_data) for x in x_data), \"Несоответствие размеров данных\"\n",
    "    return x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train, y_train = splitSamples(parsed_data)\n",
    "# x_test, y_test = splitSamples(parsed_data_for_test)\n",
    "x_train, y_train = splitSamples(train_data)\n",
    "x_test, y_test = splitSamples(parsed_data_for_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Динамический Dataset\n",
    "class DynamicNMRDataset(Dataset):\n",
    "    def __init__(self, *x_signals, y):\n",
    "        \"\"\"\n",
    "        :param x_signals: Списки сигналов (каждый размером [P, L_i], где L_i может отличаться)\n",
    "        :param y: Целевые переменные [P, N]\n",
    "        \"\"\"\n",
    "        self.x_signals = [torch.FloatTensor(x) for x in x_signals]\n",
    "        self.y = torch.FloatTensor(y)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return tuple(x[idx] for x in self.x_signals) + (self.y[idx],)\n",
    "\n",
    "# Пример использования:\n",
    "train_dataset = DynamicNMRDataset(*x_train, y=y_train)\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = DynamicNMRDataset(*x_test, y=y_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicNMRRegressor(nn.Module):\n",
    "    def __init__(self, input_dims: list, num_targets: int, conv_filters: int = 32):\n",
    "        \"\"\"\n",
    "        :param input_dims: Список размерностей для каждого типа сигнала (например, [1000, 2000] для FID и CPMG)\n",
    "        :param num_targets: Количество целевых переменных (N)\n",
    "        :param conv_filters: Базовое количество фильтров в сверточных слоях\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_experiments = len(input_dims)  # M\n",
    "        self.num_targets = num_targets          # N\n",
    "        \n",
    "        # Динамическое создание ветвей для каждого типа сигнала\n",
    "        self.branches = nn.ModuleList()\n",
    "        for dim in input_dims:\n",
    "            branch = nn.Sequential(\n",
    "                nn.Conv1d(1, conv_filters, kernel_size=5, padding=2),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool1d(4 if dim >= 2000 else 2),  # Автоматический выбор pooling\n",
    "                nn.Conv1d(conv_filters, conv_filters * 2, kernel_size=5, padding=2),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool1d(2),\n",
    "                nn.Flatten()\n",
    "            )\n",
    "            self.branches.append(branch)\n",
    "        \n",
    "        # Вычисление общего размера признаков после всех ветвей\n",
    "        self.total_features = 0\n",
    "        for i, dim in enumerate(input_dims):\n",
    "            dummy_input = torch.zeros(1, 1, dim)\n",
    "            flattened_size = self.branches[i](dummy_input).shape[1]\n",
    "            self.total_features += flattened_size\n",
    "        \n",
    "        # Финальный классификатор\n",
    "        self.final_fc = nn.Sequential(\n",
    "            nn.Linear(self.total_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_targets)\n",
    "        )\n",
    "    \n",
    "    def forward(self, *x_signals):\n",
    "        # Обработка каждого сигнала через свою ветвь\n",
    "        features = []\n",
    "        for i in range(self.num_experiments):\n",
    "            x = x_signals[i].unsqueeze(1)  # Добавляем размерность канала [B, 1, L]\n",
    "            features.append(self.branches[i](x))\n",
    "        \n",
    "        # Объединение всех признаков\n",
    "        combined = torch.cat(features, dim=1)\n",
    "        return self.final_fc(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[400, 60] 2\n",
      "Используемое устройство: cpu\n"
     ]
    }
   ],
   "source": [
    "input_dims = [len(x[0]) for x in x_train]  # Длины каждого X[i]\n",
    "num_targets = len(y_train[0])  # Количество целевых переменных (Yi)\n",
    "\n",
    "# Проверяем, что все массивы имеют одинаковую длину\n",
    "assert input_dims == [len(x[0]) for x in x_test] and num_targets == len(y_test[0]), \"Несоответствие размеров train/test\"\n",
    "\n",
    "# print(x_train)\n",
    "print(input_dims, num_targets)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Используемое устройство:\", device)\n",
    "\n",
    "model = DynamicNMRRegressor(input_dims, num_targets)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()  # Для регрессии\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Цикл обучения\n",
    "# for epoch in range(20):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "    \n",
    "#     for batch in train_dataloader:\n",
    "#         *x_batch, y_batch = batch\n",
    "#         x_batch = [x.to(device) for x in x_batch]\n",
    "#         y_batch = y_batch.to(device)\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(*x_batch)\n",
    "#         loss = criterion(outputs, y_batch)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         running_loss += loss.item()\n",
    "\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         for batch in test_dataloader:\n",
    "#             *x_batch, y_batch = batch\n",
    "#             x_batch = [x.to(device) for x in x_batch]\n",
    "#             y_batch = y_batch.to(device)\n",
    "\n",
    "    \n",
    "#     print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(train_dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Train Loss: 0.0978 | Test Loss: 0.0463 | R² Score: 0.5302\n",
      "--------------------------------------------------\n",
      "Epoch 2\n",
      "Train Loss: 0.0447 | Test Loss: 0.0286 | R² Score: 0.7079\n",
      "--------------------------------------------------\n",
      "Epoch 3\n",
      "Train Loss: 0.0314 | Test Loss: 0.0193 | R² Score: 0.8014\n",
      "--------------------------------------------------\n",
      "Epoch 4\n",
      "Train Loss: 0.0186 | Test Loss: 0.0134 | R² Score: 0.8617\n",
      "--------------------------------------------------\n",
      "Epoch 5\n",
      "Train Loss: 0.0130 | Test Loss: 0.0103 | R² Score: 0.8943\n",
      "--------------------------------------------------\n",
      "Epoch 6\n",
      "Train Loss: 0.0113 | Test Loss: 0.0096 | R² Score: 0.9018\n",
      "--------------------------------------------------\n",
      "Epoch 7\n",
      "Train Loss: 0.0102 | Test Loss: 0.0104 | R² Score: 0.8940\n",
      "--------------------------------------------------\n",
      "Epoch 8\n",
      "Train Loss: 0.0091 | Test Loss: 0.0060 | R² Score: 0.9388\n",
      "--------------------------------------------------\n",
      "Epoch 9\n",
      "Train Loss: 0.0088 | Test Loss: 0.0039 | R² Score: 0.9595\n",
      "--------------------------------------------------\n",
      "Epoch 10\n",
      "Train Loss: 0.0083 | Test Loss: 0.0042 | R² Score: 0.9578\n",
      "--------------------------------------------------\n",
      "Epoch 11\n",
      "Train Loss: 0.0079 | Test Loss: 0.0036 | R² Score: 0.9631\n",
      "--------------------------------------------------\n",
      "Epoch 12\n",
      "Train Loss: 0.0064 | Test Loss: 0.0039 | R² Score: 0.9604\n",
      "--------------------------------------------------\n",
      "Epoch 13\n",
      "Train Loss: 0.0064 | Test Loss: 0.0038 | R² Score: 0.9609\n",
      "--------------------------------------------------\n",
      "Epoch 14\n",
      "Train Loss: 0.0056 | Test Loss: 0.0049 | R² Score: 0.9506\n",
      "--------------------------------------------------\n",
      "Epoch 15\n",
      "Train Loss: 0.0060 | Test Loss: 0.0020 | R² Score: 0.9797\n",
      "--------------------------------------------------\n",
      "Epoch 16\n",
      "Train Loss: 0.0059 | Test Loss: 0.0016 | R² Score: 0.9840\n",
      "--------------------------------------------------\n",
      "Epoch 17\n",
      "Train Loss: 0.0059 | Test Loss: 0.0026 | R² Score: 0.9743\n",
      "--------------------------------------------------\n",
      "Epoch 18\n",
      "Train Loss: 0.0053 | Test Loss: 0.0015 | R² Score: 0.9847\n",
      "--------------------------------------------------\n",
      "Epoch 19\n",
      "Train Loss: 0.0055 | Test Loss: 0.0036 | R² Score: 0.9629\n",
      "--------------------------------------------------\n",
      "Epoch 20\n",
      "Train Loss: 0.0060 | Test Loss: 0.0025 | R² Score: 0.9745\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Цикл обучения\n",
    "for epoch in range(20):\n",
    "    # Фаза обучения\n",
    "    model.train()\n",
    "    train_running_loss = 0.0\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        *x_batch, y_batch = batch\n",
    "        x_batch = [x.to(device) for x in x_batch]\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(*x_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_running_loss += loss.item()\n",
    "    \n",
    "    # Фаза валидации\n",
    "    model.eval()\n",
    "    test_running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            *x_batch, y_batch = batch\n",
    "            x_batch = [x.to(device) for x in x_batch]\n",
    "            y_batch = y_batch.to(device)\n",
    "            \n",
    "            outputs = model(*x_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            test_running_loss += loss.item()\n",
    "            \n",
    "            all_preds.append(outputs.cpu().numpy())\n",
    "            all_targets.append(y_batch.cpu().numpy())\n",
    "    \n",
    "    # Собираем все предсказания и цели\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_targets = np.concatenate(all_targets, axis=0)\n",
    "    \n",
    "    # Вычисляем метрики\n",
    "    train_loss = train_running_loss / len(train_dataloader)\n",
    "    test_loss = test_running_loss / len(test_dataloader)\n",
    "    r2 = r2_score(all_targets, all_preds)\n",
    "    \n",
    "    # Выводим статистику\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f} | R² Score: {r2:.4f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для данных с M=3 (FID, CPMG, другой сигнал) и N=2\n",
    "\n",
    "input_dims = [1000, 2000, 500]  # Длины сигналов для каждого эксперимента\n",
    "# model = DynamicNMRRegressor(input_dims, num_targets=2)\n",
    "\n",
    "# Пример входных данных (P=100, M=3 сигнала разной длины)\n",
    "x_fid = torch.randn(100, 1000)    # [P, 1000]\n",
    "x_cpmg = torch.randn(100, 2000)   # [P, 2000]\n",
    "x_other = torch.randn(100, 500)   # [P, 500]\n",
    "y = torch.randn(100, 2)           # [P, 2]\n",
    "\n",
    "# Forward pass\n",
    "outputs = model(x_fid, x_cpmg, x_other)  # Автоматически обрабатывает все M сигналов\n",
    "\n",
    "# Для другого набора данных (M=2, N=3):\n",
    "# model = DynamicNMRRegressor(input_dims=[800, 1500], num_targets=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ Преимущества:\n",
    "Гибкость: Модель адаптируется к любому количеству экспериментов (M) и целей (N).\n",
    "\n",
    "Масштабируемость: Можно добавлять новые типы сигналов без изменения кода.\n",
    "\n",
    "Автоматическая адаптация: Размеры слоев вычисляются динамически.\n",
    "\n",
    "⚠️ Потенциальные проблемы:\n",
    "Производительность:\n",
    "\n",
    "Каждая ветвь обрабатывается независимо → может увеличиться время обучения.\n",
    "\n",
    "Решение: Использовать nn.ModuleDict для ветвей, если M фиксировано.\n",
    "\n",
    "Нормализация данных:\n",
    "\n",
    "Разные типы сигналов могут требовать разной предобработки.\n",
    "\n",
    "Решение: Добавить параметр scalers в модель.\n",
    "\n",
    "Интерпретируемость:\n",
    "\n",
    "Сложнее отслеживать вклад каждого типа сигнала.\n",
    "\n",
    "Решение: Визуализировать attention-веса или градиенты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Автоматический подбор гиперпараметров:\n",
    "\n",
    "def create_model(input_dims, num_targets):\n",
    "    conv_filters = 32 if max(input_dims) <= 2000 else 64\n",
    "    return DynamicNMRRegressor(input_dims, num_targets, conv_filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Динамическое построение архитектуры — это нормально и даже рекомендуется для задач с изменяющейся структурой данных. Главное:\n",
    "\n",
    "Четко определить правила генерации слоев.\n",
    "\n",
    "Проверять размерности тензоров в forward().\n",
    "\n",
    "Тестировать на крайних случаях (например, M=1, N=10)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
