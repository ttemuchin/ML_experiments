{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nh4yZ8QapEKf"
      },
      "source": [
        "= Лабораторная работа 1: TF-IDF, Word2Vec =\n",
        "\n",
        "БВТ2202, Градов Артём Николаевич\n",
        "\n",
        "Цель - реализовать алгоритм Word2Vec, рассчитать TF-IDF для произвольного набора данных(выбрано - статьи про хоккей)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pymorphy3 nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyIbUby7pVby",
        "outputId": "32f8b0ed-c571-4376-9f8c-1723096dd943"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymorphy3\n",
            "  Downloading pymorphy3-2.0.3-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Collecting dawg2-python>=0.8.0 (from pymorphy3)\n",
            "  Downloading dawg2_python-0.9.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting pymorphy3-dicts-ru (from pymorphy3)\n",
            "  Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Downloading pymorphy3-2.0.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dawg2_python-0.9.0-py3-none-any.whl (9.3 kB)\n",
            "Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymorphy3-dicts-ru, dawg2-python, pymorphy3\n",
            "Successfully installed dawg2-python-0.9.0 pymorphy3-2.0.3 pymorphy3-dicts-ru-2.4.417150.4580142\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIFMFJ18pXie",
        "outputId": "be87f869-c450-4a77-fcd4-8e48fccfd0c4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXF-D6mapEKg",
        "outputId": "f2c4e0d3-d5cc-4aa0-d7ac-bdef10dc9d66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "import pymorphy3\n",
        "import pandas as pd\n",
        "\n",
        "from tqdm import tqdm\n",
        "from nltk.stem.snowball import RussianStemmer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "tqdm.pandas()\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IHuolXdgpEKh"
      },
      "outputs": [],
      "source": [
        "train_dataframe = pd.read_csv(\"/content/drive/MyDrive/train.csv\").iloc[:2000]\n",
        "valid_dataframe = pd.read_csv(\"/content/drive/MyDrive/test.csv\").iloc[:400]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9Q3IdIiYpEKh"
      },
      "outputs": [],
      "source": [
        "def preproc(text):\n",
        "  text = text.lower()\n",
        "  text = re.sub(r\"([^\\w\\s]\\s)\", r\" \\1\", text)\n",
        "  text = re.sub(r\"(\\d)\\.\", r\"\\1 . \", text)\n",
        "  text = re.sub(r\"\\s+\", r\" \", text)\n",
        "  text = text.strip()\n",
        "  text = text.split(\" \")\n",
        "# это лемантизация. вариант со стеммингом еще дольше\n",
        "  morph = pymorphy3.MorphAnalyzer()\n",
        "  text = [morph.parse(word)[0].normal_form for word in text]\n",
        "\n",
        "  return text\n",
        "# получили набор токенов\n",
        "\n",
        "def slicingWindow(sequence):\n",
        "  pairs = []\n",
        "  for index, word in enumerate(sequence):\n",
        "    for jndex in [-2, -1, 1, 2]:\n",
        "      if 0 <= index+jndex < len(sequence):\n",
        "        pairs.append((sequence[index], sequence[index+jndex]))\n",
        "  return pairs\n",
        "# все пары слов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQ1uvxEcpEKh",
        "outputId": "2f0283b6-ed0f-4153-90c2-145cb1440f1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2000/2000 [02:55<00:00, 11.39it/s]\n",
            "100%|██████████| 2000/2000 [00:00<00:00, 16381.47it/s]\n",
            "100%|██████████| 400/400 [00:35<00:00, 11.35it/s]\n",
            "100%|██████████| 400/400 [00:00<00:00, 10377.83it/s]\n"
          ]
        }
      ],
      "source": [
        "train_pairs = train_dataframe[\"text\"].progress_apply(preproc).progress_apply(slicingWindow).explode().tolist()\n",
        "valid_pairs = valid_dataframe[\"text\"].progress_apply(preproc).progress_apply(slicingWindow).explode().tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FKxwRlKApEKi"
      },
      "outputs": [],
      "source": [
        "class PairDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, pairs):\n",
        "    self.pairs = pairs\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.pairs)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.pairs[index][0], self.pairs[index][1]\n",
        "\n",
        "train_dataset = PairDataset(train_pairs)\n",
        "valid_dataset = PairDataset(valid_pairs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6XP9w6FpEKi",
        "outputId": "6616863e-042a-4381-9a35-cd627ee97199"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2400/2400 [03:44<00:00, 10.71it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([15548, 16242,  9656, 18258])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# простой токенизатор для слов\n",
        "# есть два словаря - это не индексы слова в предложении, это все слова датасета и индексы для них\n",
        "\n",
        "class Tokenizer:\n",
        "  def __init__(self, set_of_words):\n",
        "    self.__word2index = {} #слово в токен\n",
        "    self.__index2word = {} #токен в слово\n",
        "    for index, value in enumerate(set_of_words):\n",
        "      self.__word2index[value] = index\n",
        "      self.__index2word[index] = value\n",
        "\n",
        "  def __call__(self, sequence):\n",
        "    tokens = []\n",
        "    for word in sequence:\n",
        "      try:\n",
        "        tokens.append(self.__word2index[word.lower()])\n",
        "      except:\n",
        "        raise ValueError(\"Такого слова нет в токенизаторе\")\n",
        "    return torch.tensor(tokens)\n",
        "\n",
        "  def decode(self, sequence):\n",
        "    words = []\n",
        "    for index in sequence:\n",
        "      try:\n",
        "        words.append(self.__index2word[index])\n",
        "      except:\n",
        "        raise ValueError(\"Такого токена нет в токенизаторе\")\n",
        "    return words\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.__word2index)\n",
        "\n",
        "tokenizer_dataframe = pd.concat([train_dataframe[\"text\"], valid_dataframe[\"text\"]])\n",
        "tokenizer = Tokenizer(set(tokenizer_dataframe.progress_apply(preproc).explode().tolist()))\n",
        "tokenizer(preproc(\"Привет меня зовут Руслан\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47Z4H8W7pEKi",
        "outputId": "7f5bc504-52d5-4af8-94e8-a51c45fb67f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "class word2vec(nn.Module):\n",
        "  def __init__(self, num_dict, hidden):\n",
        "    super().__init__()\n",
        "    self.encode = nn.Embedding(num_dict, hidden)\n",
        "    self.decode = nn.Linear(hidden, num_dict)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.encode(x)\n",
        "    out = self.decode(out)\n",
        "    return out\n",
        "\n",
        "num_dict = 18728\n",
        "hidden = 300\n",
        "batch_size = 16\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "model = word2vec(num_dict, hidden).to(device)\n",
        "optim = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "epochs = range(10)\n",
        "\n",
        "train_loss = []\n",
        "test_loss = []\n",
        "train_acc = []\n",
        "test_acc = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qGyo_UZpEKi",
        "outputId": "9e47c944-a3e1-4cf8-8715-9554397643f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch = 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 31061/31061 [03:29<00:00, 148.02it/s, Loss=8.2, Accuracy=0.0625]\n",
            "100%|██████████| 6606/6606 [00:13<00:00, 496.46it/s, Loss=9.9, Accuracy=0.125]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train_loss: 7.9384184085541\n",
            "Valid_loss: 8.060679777261166\n",
            "Train_acc: 0.045728566369402146\n",
            "Valid_acc: 0.052736148955495006\n",
            "Epoch = 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 31061/31061 [03:31<00:00, 146.68it/s, Loss=8.46, Accuracy=0.0625]\n",
            "100%|██████████| 6606/6606 [00:13<00:00, 495.36it/s, Loss=10.4, Accuracy=0.125]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train_loss: 7.139791352186771\n",
            "Valid_loss: 8.262530702372374\n",
            "Train_acc: 0.05867285341746885\n",
            "Valid_acc: 0.0542310021192855\n",
            "Epoch = 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 31061/31061 [03:30<00:00, 147.28it/s, Loss=6.45, Accuracy=0.0625]\n",
            "100%|██████████| 6606/6606 [00:13<00:00, 495.97it/s, Loss=10.5, Accuracy=0.0625]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train_loss: 6.954505346258979\n",
            "Valid_loss: 8.324405559654709\n",
            "Train_acc: 0.06123434532049837\n",
            "Valid_acc: 0.05527172267635483\n",
            "Epoch = 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 31061/31061 [03:30<00:00, 147.21it/s, Loss=7.37, Accuracy=0.0625]\n",
            "100%|██████████| 6606/6606 [00:13<00:00, 501.45it/s, Loss=10.4, Accuracy=0.125]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train_loss: 6.844768223116669\n",
            "Valid_loss: 8.35822268586934\n",
            "Train_acc: 0.06302316409645536\n",
            "Valid_acc: 0.057305858310626706\n",
            "Epoch = 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 31061/31061 [03:31<00:00, 147.08it/s, Loss=8.36, Accuracy=0]\n",
            "100%|██████████| 6606/6606 [00:12<00:00, 508.36it/s, Loss=10.4, Accuracy=0.125]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train_loss: 6.759256411134527\n",
            "Valid_loss: 8.386230489574633\n",
            "Train_acc: 0.06454235214577766\n",
            "Valid_acc: 0.05696525885558583\n",
            "Epoch = 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 31061/31061 [03:31<00:00, 146.94it/s, Loss=8.02, Accuracy=0]\n",
            "100%|██████████| 6606/6606 [00:13<00:00, 491.36it/s, Loss=10.5, Accuracy=0.125]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train_loss: 6.687431556267147\n",
            "Valid_loss: 8.406682427150352\n",
            "Train_acc: 0.06628891536009787\n",
            "Valid_acc: 0.05647328186497124\n",
            "Epoch = 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 31061/31061 [03:30<00:00, 147.69it/s, Loss=7.4, Accuracy=0.0625]\n",
            "100%|██████████| 6606/6606 [00:13<00:00, 491.69it/s, Loss=10.5, Accuracy=0.125]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train_loss: 6.620458077019639\n",
            "Valid_loss: 8.423893770917488\n",
            "Train_acc: 0.0675545700395995\n",
            "Valid_acc: 0.05690849227974568\n",
            "Epoch = 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 31061/31061 [03:30<00:00, 147.54it/s, Loss=6.24, Accuracy=0.0625]\n",
            "100%|██████████| 6606/6606 [00:13<00:00, 504.68it/s, Loss=10.4, Accuracy=0.125]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train_loss: 6.557631070183785\n",
            "Valid_loss: 8.441949456740682\n",
            "Train_acc: 0.06887254112874666\n",
            "Valid_acc: 0.05669088707235846\n",
            "Epoch = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 31061/31061 [03:26<00:00, 150.58it/s, Loss=6.99, Accuracy=0.0625]\n",
            "100%|██████████| 6606/6606 [00:13<00:00, 506.77it/s, Loss=10.5, Accuracy=0.125]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train_loss: 6.497442089940625\n",
            "Valid_loss: 8.46160705759783\n",
            "Train_acc: 0.06995911271369241\n",
            "Valid_acc: 0.0565489706327581\n",
            "Epoch = 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 31061/31061 [03:25<00:00, 150.83it/s, Loss=6.63, Accuracy=0.0625]\n",
            "100%|██████████| 6606/6606 [00:13<00:00, 501.31it/s, Loss=10.5, Accuracy=0.125]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train_loss: 6.438648970127643\n",
            "Valid_loss: 8.476053698078054\n",
            "Train_acc: 0.07148433727181996\n",
            "Valid_acc: 0.056321904329397515\n"
          ]
        }
      ],
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "for epoch in epochs:\n",
        "  print(f\"Epoch = {epoch}\")\n",
        "\n",
        "  train_loss_mean = 0\n",
        "  test_loss_mean = 0\n",
        "  train_acc_mean = 0\n",
        "  test_acc_mean = 0\n",
        "\n",
        "  for data, target in (pbar := tqdm(train_dataloader)):\n",
        "    optim.zero_grad()\n",
        "\n",
        "    data = tokenizer(data).to(device)\n",
        "    target = tokenizer(target).to(device)\n",
        "\n",
        "    out = model(data)\n",
        "\n",
        "    b_train_acc = float((out.argmax(dim=1) == target).sum()/batch_size)\n",
        "    train_acc_mean += b_train_acc\n",
        "\n",
        "    target = F.one_hot(target, num_classes=num_dict).float()\n",
        "    loss = loss_fn(out, target)\n",
        "    loss.backward()\n",
        "    train_loss_mean += loss.item()\n",
        "\n",
        "    optim.step()\n",
        "\n",
        "    pbar.set_postfix({\"Loss\": loss.item(), \"Accuracy\": b_train_acc}, refresh=True)\n",
        "\n",
        "  for data, target in (pbar := tqdm(valid_dataloader)):\n",
        "    with torch.no_grad():\n",
        "      data = tokenizer(data).to(device)\n",
        "      target = tokenizer(target).to(device)\n",
        "\n",
        "      out = model(data)\n",
        "      b_test_acc = float((out.argmax(dim=1) == target).sum()/batch_size)\n",
        "      test_acc_mean += b_test_acc\n",
        "\n",
        "      target = F.one_hot(target, num_classes=num_dict).float()\n",
        "      loss = loss_fn(out, target)\n",
        "      test_loss_mean += loss.item()\n",
        "\n",
        "      pbar.set_postfix({\"Loss\": loss.item(), \"Accuracy\": b_test_acc}, refresh=True)\n",
        "\n",
        "  train_loss.append(train_loss_mean/len(train_dataloader))\n",
        "  test_loss.append(test_loss_mean/len(valid_dataloader))\n",
        "  train_acc.append(train_acc_mean/len(train_dataloader))\n",
        "  test_acc.append(test_acc_mean/len(valid_dataloader))\n",
        "\n",
        "  print(f\"Train_loss: {train_loss_mean/len(train_dataloader)}\")\n",
        "  print(f\"Valid_loss: {test_loss_mean/len(valid_dataloader)}\")\n",
        "  print(f\"Train_acc: {train_acc_mean/len(train_dataloader)}\")\n",
        "  print(f\"Valid_acc: {test_acc_mean/len(valid_dataloader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8N3y4whpEKi"
      },
      "source": [
        "TF-IDF\n",
        "\n",
        "term frequency, inverse document frequency / количество слов в документе, количество документов с этим словом\n",
        "\n",
        "TF = (N экземпляров конкретного слова в документе) / (N слов в документе)\n",
        "\n",
        "IDF = log((N документов) / (N документов с этим словом))\n",
        "\n",
        "TF-IDF = TF * IDF => вес слова в каждом документе\n",
        "\n",
        "Так оцениваем важность слова в документе относительно всего корпуса\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "2Y5QwwN9pEKi",
        "outputId": "3d3fa242-7615-4e8d-b320-35a824124a4b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'rec.sport.hockey'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-246e1b84f1e1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfindTf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;31m# print(tf) #'rally': defaultdict(<class 'int'>, {'54776': 0.03225806451612903})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'when'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'52550'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-246e1b84f1e1>\u001b[0m in \u001b[0;36mfindTf\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# подсчеты\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mdoc_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin-1'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'rec.sport.hockey'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "path = 'rec.sport.hockey'\n",
        "\n",
        "def findTf(path):\n",
        "    tf = defaultdict(lambda: defaultdict(int))  # созд. ключи с 0 вместо ошибки\n",
        "    doc_word_counts = {}  # id: N\n",
        "    document_count = 0 #99\n",
        "\n",
        "    # подсчеты\n",
        "    for filename in os.listdir(path):\n",
        "        doc_id = filename\n",
        "        with open(os.path.join(path, filename), 'r', encoding='latin-1') as file:\n",
        "            # content = file.readlines()\n",
        "            content = file.read().lower()\n",
        "            lines = content.splitlines()\n",
        "            word_count = 0\n",
        "    # плохо ли то что слова попадаются 1-3 раза? размер доков разный?\n",
        "            for line in lines:\n",
        "                if re.match(r\">>\\w|>\\w|\\|>\\w|-\\w\", line):\n",
        "                    words = line.split()\n",
        "                    words = [word.strip('<>-') for word in words]\n",
        "                    words = [word.rstrip('.,!?') for word in words]\n",
        "                    words = [word.lower() for word in words]\n",
        "                    word_count += len(words)\n",
        "                    for word in words:\n",
        "                        tf[word][doc_id] += 1\n",
        "\n",
        "            doc_word_counts[doc_id] = word_count\n",
        "            document_count += 1\n",
        "\n",
        "    # tf\n",
        "    for word, doc_counts in tf.items():\n",
        "        for doc_id, count in doc_counts.items():\n",
        "            tf[word][doc_id] = count / doc_word_counts[doc_id]\n",
        "\n",
        "    return tf, document_count\n",
        "\n",
        "def findIdf(tf, document_count):\n",
        "    idf = {}\n",
        "    for word in tf:\n",
        "        docs_include = len(tf[word])\n",
        "        idf[word] = np.log(document_count / (docs_include + 1))\n",
        "    return idf\n",
        "\n",
        "def findTfidf(tf, idf):\n",
        "    tfidf = defaultdict(lambda: defaultdict(float))  # tfidf[word][doc_id]\n",
        "    for word, doc_counts in tf.items():\n",
        "        for doc_id, tf_value in doc_counts.items():\n",
        "            tfidf[word][doc_id] = float(np.round(tf_value * idf[word], 7))\n",
        "    return tfidf\n",
        "\n",
        "\n",
        "tf, document_count = findTf(path)\n",
        "# print(tf) #'rally': defaultdict(<class 'int'>, {'54776': 0.03225806451612903})\n",
        "print(tf['when']['52550'], \"\\n\")\n",
        "\n",
        "idf = findIdf(tf, document_count)\n",
        "\n",
        "tfidf = findTfidf(tf, idf)\n",
        "\n",
        "for word, doc_counts in tfidf.items():\n",
        "    if len(dict(doc_counts)) >= 3:\n",
        "        print(f'TF-IDF слова \"{word}\": {dict(doc_counts)}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPHXPPXypEKj"
      },
      "source": [
        "PyTorch\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rppl5dc-pEKj"
      },
      "source": [
        "### 1. class SkipGramModel(nn.Module)\n",
        "\n",
        "`SkipGramModel` - это класс, который наследуется от `nn.Module`, базового класса для всех нейронных сетей в PyTorch. Он предоставляет структуру для создания модели обучения.\n",
        "\n",
        "- **Определение модели**: Внутри конструктора `__init__` можно определить все компоненты модели, такие как слои, которые будут использоваться.\n",
        "- **Метод forward**: Определяет, как проходить данные через модель. В данном случае он описывает, как данные обрабатываются в модели Skip-Gram, чтобы предсказать вероятности контекстных слов на основе целевых слов.\n",
        "\n",
        "### 2. nn.Embedding(vocab_size, embedding_size)\n",
        "\n",
        "`nn.Embedding` - это специальный слой, который используется для представления категориальных данных (например, слов) в виде плотных векторных представлений.\n",
        "\n",
        "- **vocab_size**: Количество уникальных слов в словаре модели. Это будет размерность входного пространства.\n",
        "- **embedding_size**: Размерность создаваемых векторов для каждого слова. Это влияет на общее качество представления слов и объём обучаемых параметров.\n",
        "\n",
        "Пример: `nn.Embedding(1000, 100)` создаст матрицу весов 1000x100, где каждая строка представляет слово в виде 100-мерного вектора.\n",
        "\n",
        "### 3. torch.matmul(in_embeds, out_embeds.t())\n",
        "\n",
        "`torch.matmul` - это функция для выполнения матричного умножения между двумя тензорами.\n",
        "\n",
        "- **in_embeds**: Векторные представления целевых слов, полученные из входного слоя (`in_embeddings`).\n",
        "- **out_embeds.t()**: Транспонированные векторные представления всех слов, используемые в качестве выходных. Транспонирование нужно для того, чтобы размеры матриц совпадали для перемножения.\n",
        "\n",
        "Эта операция позволяет эффективно вычислить предсказания модели, сравнивая векторы целевых слов с векторами всех контекстных слов, получая вероятностную оценку.\n",
        "\n",
        "### 4. optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "optimizer.step()\n",
        "\n",
        "- **optim.SGD**: Это реализация стохастического градиентного спуска (SGD) как метода оптимизации. Он обновляет параметры модели в направлении, которое минимизирует функцию потерь.\n",
        "- **model.parameters()**: Метод, который возвращает все параметры (веса) модели, которые подлежат обучению.\n",
        "- **lr=learning_rate**: Научная скорость, по которой оптимизатор обновляет параметры модели.\n",
        "- `optimizer.step()`: Метод, который обновляет параметры модели, используя значения накапливаемых градиентов.\n",
        "\n",
        "### 5. loss_function = nn.CrossEntropyLoss()\n",
        "loss = loss_function(scores, context_tensor)\n",
        "loss.backward()\n",
        "\n",
        "- **nn.CrossEntropyLoss**: Функция потерь, которая будет использоваться для обучения модели. Она сочетает в себе softmax и потерю логарифмического распределения для многоклассовой классификации.\n",
        "- **scores**: Результаты предсказания модели (логиты) для целевого слова.\n",
        "- **context_tensor**: Индексы контекстных слов, которые мы пытаемся предсказать. Эта информация используется, чтобы вычислить, насколько хорошо модель предсказывает слова на основе целевого.\n",
        "- `loss.backward()`: Запускает обратное распространение ошибки, вычисляя градиенты для всех обучаемых параметров модели.\n",
        "\n",
        "### 6. target_tensor = torch.tensor([target], dtype=torch.long)\n",
        "context_tensor = torch.tensor([context], dtype=torch.long)\n",
        "\n",
        "- **torch.tensor**: Создает новый тензор из данных, переданных в виде списка или массива.\n",
        "- **target_tensor**: Он содержит индекс целевого слова, которое мы берем как вход на шаге обучения. Его размерность - это 1.\n",
        "- **context_tensor**: Он содержит индексы контекстных слов, которые модель пытается предсказать для данного целевого слова. Также размерность - 1.\n",
        "\n",
        "Использование `dtype=torch.long` означает, что элементы тензора будут целыми числами, что является необходимым для индексов, так как индексы слов в `nn.Embedding` должны быть целыми.\n",
        "\n",
        "Таким образом, каждый элемент кода выполняет важную функцию в процессе построения и обучения модели, от создания векторных представлений до управления обновлением параметров."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}